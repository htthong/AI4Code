{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6afc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "pd.options.display.width = 180\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "BERT_PATH = \".\"\n",
    "\n",
    "data_dir = Path('./AI4Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ffcb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train NBs: 100%|████████████████████████████████████████████████████████████████| 20000/20000 [01:19<00:00, 252.02it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 20000\n",
    "\n",
    "\n",
    "def read_notebook(path):\n",
    "    return (\n",
    "        pd.read_json(\n",
    "            path,\n",
    "            dtype={'cell_type': 'category', 'source': 'str'})\n",
    "        .assign(id=path.stem)\n",
    "        .rename_axis('cell_id')\n",
    "    )\n",
    "\n",
    "\n",
    "paths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]\n",
    "notebooks_train = [\n",
    "    read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
    "]\n",
    "df = (\n",
    "    pd.concat(notebooks_train)\n",
    "    .set_index('id', append=True)\n",
    "    .swaplevel()\n",
    "    .sort_index(level='id', sort_remaining=False)\n",
    ")\n",
    "df.source = df.source.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e1486c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huynh\\AppData\\Local\\Temp\\ipykernel_22900\\2875427560.py:1: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  df_orders = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "df_orders = pd.read_csv(\n",
    "    data_dir / 'train_orders.csv',\n",
    "    index_col='id',\n",
    "    squeeze=True,\n",
    ").str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92312426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(base, derived):\n",
    "    return [base.index(d) for d in derived]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aac4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_ = df_orders.to_frame().join(\n",
    "    df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n",
    "    how='right',\n",
    ")\n",
    "\n",
    "ranks = {}\n",
    "for id_, cell_order, cell_id in df_orders_.itertuples():\n",
    "    ranks[id_] = {'cell_id': cell_id, 'rank': get_ranks(cell_order, cell_id)}\n",
    "\n",
    "df_ranks = (\n",
    "    pd.DataFrame\n",
    "    .from_dict(ranks, orient='index')\n",
    "    .rename_axis('id')\n",
    "    .apply(pd.Series.explode)\n",
    "    .set_index('cell_id', append=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09aee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4cb1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>rank</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>1862f0a6</td>\n",
       "      <td>code</td>\n",
       "      <td># this python 3 environment comes with many helpful analytics libraries installed\\n# it is defined by the kaggle/pyt...</td>\n",
       "      <td>0</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>2a9e43d6</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd\\nimport random\\n\\nfrom sklearn.model_selection import train_test_split, cros...</td>\n",
       "      <td>2</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>038b763d</td>\n",
       "      <td>code</td>\n",
       "      <td>import warnings\\nwarnings.filterwarnings('ignore')</td>\n",
       "      <td>4</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>2eefe0ef</td>\n",
       "      <td>code</td>\n",
       "      <td>matplotlib.rcparams.update({'font.size': 14})</td>\n",
       "      <td>6</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00001756c60be8</td>\n",
       "      <td>0beab1cd</td>\n",
       "      <td>code</td>\n",
       "      <td>def evaluate_preds(train_true_values, train_pred_values, test_true_values, test_pred_values):\\n    print(\"train r2:\\...</td>\n",
       "      <td>8</td>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916551</th>\n",
       "      <td>249632a403a0d4</td>\n",
       "      <td>c33633b1</td>\n",
       "      <td>markdown</td>\n",
       "      <td>### 4. top 10 publisher by:\\n*  na sales: nintendo.\\n*  eu sales: nintendo.\\n*  jp sales: nintendo.\\n*  other sales:...</td>\n",
       "      <td>47</td>\n",
       "      <td>bee66634</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916552</th>\n",
       "      <td>249632a403a0d4</td>\n",
       "      <td>a3443531</td>\n",
       "      <td>markdown</td>\n",
       "      <td>### top publishers releases over years</td>\n",
       "      <td>62</td>\n",
       "      <td>bee66634</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916553</th>\n",
       "      <td>249632a403a0d4</td>\n",
       "      <td>8e0d6d03</td>\n",
       "      <td>markdown</td>\n",
       "      <td>### 2. top 10 genre by:\\n*  global sales: action.\\n*  na sales: action.\\n*  eu sales: action.\\n*  jp sales: role-pla...</td>\n",
       "      <td>31</td>\n",
       "      <td>bee66634</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916554</th>\n",
       "      <td>249632a403a0d4</td>\n",
       "      <td>57f3400f</td>\n",
       "      <td>markdown</td>\n",
       "      <td>### 3) data visualization\\n\\ntasks:\\n\\nmake a report of:\\n\\n* top 10 game by total reveneu worldwide and by eu, jp, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>bee66634</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916555</th>\n",
       "      <td>249632a403a0d4</td>\n",
       "      <td>f917f3f5</td>\n",
       "      <td>markdown</td>\n",
       "      <td>### top genres releases over years</td>\n",
       "      <td>59</td>\n",
       "      <td>bee66634</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916556 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id   cell_id cell_type                                                                                                                   source rank  \\\n",
       "0       00001756c60be8  1862f0a6      code  # this python 3 environment comes with many helpful analytics libraries installed\\n# it is defined by the kaggle/pyt...    0   \n",
       "1       00001756c60be8  2a9e43d6      code  import numpy as np\\nimport pandas as pd\\nimport random\\n\\nfrom sklearn.model_selection import train_test_split, cros...    2   \n",
       "2       00001756c60be8  038b763d      code                                                                       import warnings\\nwarnings.filterwarnings('ignore')    4   \n",
       "3       00001756c60be8  2eefe0ef      code                                                                            matplotlib.rcparams.update({'font.size': 14})    6   \n",
       "4       00001756c60be8  0beab1cd      code  def evaluate_preds(train_true_values, train_pred_values, test_true_values, test_pred_values):\\n    print(\"train r2:\\...    8   \n",
       "...                ...       ...       ...                                                                                                                      ...  ...   \n",
       "916551  249632a403a0d4  c33633b1  markdown  ### 4. top 10 publisher by:\\n*  na sales: nintendo.\\n*  eu sales: nintendo.\\n*  jp sales: nintendo.\\n*  other sales:...   47   \n",
       "916552  249632a403a0d4  a3443531  markdown                                                                                   ### top publishers releases over years   62   \n",
       "916553  249632a403a0d4  8e0d6d03  markdown  ### 2. top 10 genre by:\\n*  global sales: action.\\n*  na sales: action.\\n*  eu sales: action.\\n*  jp sales: role-pla...   31   \n",
       "916554  249632a403a0d4  57f3400f  markdown  ### 3) data visualization\\n\\ntasks:\\n\\nmake a report of:\\n\\n* top 10 game by total reveneu worldwide and by eu, jp, ...   16   \n",
       "916555  249632a403a0d4  f917f3f5  markdown                                                                                       ### top genres releases over years   59   \n",
       "\n",
       "       ancestor_id parent_id  \n",
       "0         945aea18       NaN  \n",
       "1         945aea18       NaN  \n",
       "2         945aea18       NaN  \n",
       "3         945aea18       NaN  \n",
       "4         945aea18       NaN  \n",
       "...            ...       ...  \n",
       "916551    bee66634       NaN  \n",
       "916552    bee66634       NaN  \n",
       "916553    bee66634       NaN  \n",
       "916554    bee66634       NaN  \n",
       "916555    bee66634       NaN  \n",
       "\n",
       "[916556 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index().merge(df_ranks, on=[\"id\", \"cell_id\"]).merge(df_ancestors, on=[\"id\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fcfca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "# import fasttext\n",
    "\n",
    "re_sc_ch = re.compile(r'\\P{L}+')\n",
    "re_sg_ch = re.compile(r' +\\p{L} +')\n",
    "re_sg_ch_st = re.compile(r'^\\p{L} +')\n",
    "re_mul_sp =  re.compile(r' +')\n",
    "re_pre_b = re.compile(r'^b +')\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove all the special characters\n",
    "        document = re_sc_ch.sub(' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re_sg_ch.sub(' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re_sg_ch_st.sub(' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re_mul_sp.sub(' ', document)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document =re_pre_b.sub('', document)\n",
    "\n",
    "        return document\n",
    "\n",
    "    \n",
    "def preprocess_df(df):\n",
    "    \"\"\"\n",
    "    This function is for processing sorce of notebook\n",
    "    returns preprocessed dataframe\n",
    "    \"\"\"\n",
    "    return [preprocess_text(message) for message in df.source]\n",
    "\n",
    "df.source = df.source.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc56d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cellid_source = dict(zip(df['cell_id'].values, df['source'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567a4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained('./Model/Pre-trained/tokenizer')\n",
    "#model = AutoModelWithLMHead.from_pretrained('./Model/Pre-trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1edc436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 20000/20000 [00:29<00:00, 678.43it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_triplet(df, mode='train'):\n",
    "    triplets = []\n",
    "    ids = df.id.unique()\n",
    "    random_drop = np.random.random(size=10000)>0.9\n",
    "    count = 0\n",
    "\n",
    "    for id, df_tmp in tqdm(df.groupby('id')):\n",
    "        df_tmp_markdown = df_tmp[df_tmp['cell_type']=='markdown']\n",
    "\n",
    "        df_tmp_code = df_tmp[df_tmp['cell_type']=='code']\n",
    "        df_tmp_code_rank = df_tmp_code['rank'].values\n",
    "        df_tmp_code_cell_id = df_tmp_code['cell_id'].values\n",
    "\n",
    "        for cell_id, rank in df_tmp_markdown[['cell_id', 'rank']].values:\n",
    "            labels = np.array([(r==(rank+1)) for r in df_tmp_code_rank]).astype('int')\n",
    "\n",
    "            for cid, label in zip(df_tmp_code_cell_id, labels):\n",
    "                count += 1\n",
    "                if label==1:\n",
    "                    triplets.append( [cell_id, cid, label] )\n",
    "                    # triplets.append( [cid, cell_id, label] )\n",
    "                elif mode == 'test':\n",
    "                    triplets.append( [cell_id, cid, label] )\n",
    "                    # triplets.append( [cid, cell_id, label] )\n",
    "                elif random_drop[count%10000]:\n",
    "                    triplets.append( [cell_id, cid, label] )\n",
    "                    # triplets.append( [cid, cell_id, label] )\n",
    "\n",
    "    return triplets\n",
    "\n",
    "triplets = generate_triplet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e806bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "MAX_LEN = 128\n",
    "    \n",
    "class MarkdownModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarkdownModel, self).__init__()\n",
    "        self.distill_bert = AutoModel.from_pretrained(\"./Model\")\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.top = torch.nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = self.dropout(x)\n",
    "        x = self.top(x[:, 0, :])\n",
    "        x = torch.sigmoid(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fca30ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MarkdownDataset(Dataset):\n",
    "    def __init__(self, df, max_len, mode='train'):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"./Model/tokenizer\")\n",
    "        self.mode=mode\n",
    "       \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df[index]\n",
    "        label = row[-1]\n",
    "        txt = dict_cellid_source[row[0]] + '[SEP]' + dict_cellid_source[row[1]]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            txt,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "        return ids, mask, torch.FloatTensor([label])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "train_ds = MarkdownDataset(triplets, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6038d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch):\n",
    "    if epoch < 1:\n",
    "        lr = 3e-5\n",
    "    elif epoch < 3:\n",
    "        lr = 1e-5\n",
    "    elif epoch < 5:\n",
    "        lr = 5e-6\n",
    "    else:\n",
    "        lr = 1e-7\n",
    "    for p in optimizer.param_groups:\n",
    "        p['lr'] = lr\n",
    "    return lr\n",
    "    \n",
    "    \n",
    "def get_optimizer(net):\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=3e-4, betas=(0.9, 0.999),\n",
    "                                 eps=1e-8 ) #1e-08)\n",
    "    return optimizer\n",
    "\n",
    "BS = 128 \n",
    "#NW = 8\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BS, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92b4b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./Model were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./Model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "def read_data(data):\n",
    "    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n",
    "\n",
    "def validate(model, val_loader, mode='train'):\n",
    "    model.eval()\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "            pred = model(inputs[0], inputs[1])\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            if mode=='train':\n",
    "                labels.append(target.detach().cpu().numpy().ravel())\n",
    "    if mode=='test':\n",
    "        return np.concatenate(preds)\n",
    "    else:\n",
    "        return np.concatenate(labels), np.concatenate(preds)\n",
    "\n",
    "def train(model, train_loader, epochs, Type='markdown'):\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    optimizer = get_optimizer(model)\n",
    "\n",
    "    mixed_precision = True\n",
    "    try:  \n",
    "        from apex import amp\n",
    "    except:\n",
    "        mixed_precision = False  # not installed\n",
    "        \n",
    "    # model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=1)\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    #criterion = torch.nn.BCELoss()\n",
    "    \n",
    "    for e in range(epochs):   \n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, position=0, leave=True)\n",
    "        lr = adjust_lr(optimizer, e)\n",
    "        loss_list = []\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(inputs[0], inputs[1])\n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            labels.append(target.detach().cpu().numpy().ravel())\n",
    "            \n",
    "            avg_loss = np.round(np.mean(loss_list), 4)\n",
    "            tbar.set_description(f\"Epoch {e} Loss: {avg_loss} lr: {lr}\")\n",
    "        \n",
    "        output_model_file = f\"./my_model/my_own_model_{e}.bin\"\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = MarkdownModel()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.1363 lr: 3e-05: 100%|██████████████████████████████████████████| 52194/52194 [2:11:00<00:00,  6.64it/s]\n",
      "Epoch 1 Loss: 0.1362 lr: 1e-05: 100%|██████████████████████████████████████████| 52194/52194 [2:10:39<00:00,  6.66it/s]\n",
      "Epoch 2 Loss: 0.1362 lr: 1e-05:  40%|████████████████▋                         | 20747/52194 [51:33<1:18:45,  6.65it/s]"
     ]
    }
   ],
   "source": [
    "model = train(model, train_loader, epochs=15, Type='markdown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a815cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5abb34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size = 128\n",
    "embedding_dim = 16\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_vocab_size, embedding_dim)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "713b3565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 16)          2048      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,048\n",
      "Trainable params: 2,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe07f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_4_input'), name='embedding_4_input', description=\"created by layer 'embedding_4_input'\"), but it was called on an input with incompatible shape (None,).\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03175068,  0.04518476,  0.03099047,  0.03902406, -0.0238451 ,\n",
       "         0.01022248,  0.04638659,  0.00112636, -0.02454869, -0.02420332,\n",
       "         0.01812441, -0.02569896, -0.00101122,  0.03786495,  0.03084195,\n",
       "        -0.04268376],\n",
       "       [ 0.01147168,  0.02028776,  0.01787443,  0.02878406,  0.03316477,\n",
       "        -0.03730999, -0.02772543,  0.02157864, -0.04378268, -0.04534464,\n",
       "         0.01430156, -0.04093121, -0.04419219,  0.01793952,  0.04641494,\n",
       "        -0.02804509],\n",
       "       [-0.00246056,  0.03388472,  0.01325028, -0.01709906,  0.02511751,\n",
       "         0.02343695, -0.03716427, -0.0100871 , -0.04942725,  0.00847252,\n",
       "         0.01380309, -0.01255905,  0.02336795,  0.01358635,  0.02113799,\n",
       "        -0.03850905],\n",
       "       [-0.01261082, -0.00207077, -0.00695227,  0.00687863,  0.00174991,\n",
       "        -0.00712912, -0.03085891,  0.02559337,  0.00947325,  0.03509101,\n",
       "         0.01380303, -0.02599156,  0.01021805,  0.03814136,  0.01850437,\n",
       "         0.00975385],\n",
       "       [ 0.00257504,  0.02435377, -0.02083349,  0.0214943 , -0.04740904,\n",
       "        -0.02052402, -0.02571698,  0.00986791, -0.04629034, -0.04029381,\n",
       "        -0.02886068,  0.04766998, -0.03978161, -0.01580744,  0.03728249,\n",
       "        -0.02543737]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array([5,1,2,3,4])\n",
    "model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b841e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size = 128\n",
    "embedding_dim = 16\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_vocab_size, embedding_dim),\n",
    "    tf.keras.layers.GRU(256, dropout= 0.2),\n",
    "    tf.keras.layers.Dense(64, activation='ReLU'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a143af10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 16)          2048      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 256)               210432    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 228,993\n",
      "Trainable params: 228,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cb649f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 1024])\n",
      "linear_relu_stack.0.weight_ih_l0   torch.Size([192, 1024])\n",
      "torch.Size([192, 64])\n",
      "linear_relu_stack.0.weight_hh_l0   torch.Size([192, 64])\n",
      "torch.Size([192])\n",
      "linear_relu_stack.0.bias_ih_l0   torch.Size([192])\n",
      "torch.Size([192])\n",
      "linear_relu_stack.0.bias_hh_l0   torch.Size([192])\n",
      "torch.Size([64, 128])\n",
      "linear_relu_stack.1.weight   torch.Size([64, 128])\n",
      "torch.Size([64])\n",
      "linear_relu_stack.1.bias   torch.Size([64])\n",
      "torch.Size([64, 64])\n",
      "linear_relu_stack.2.weight   torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "linear_relu_stack.2.bias   torch.Size([64])\n",
      "torch.Size([64, 64])\n",
      "linear_relu_stack.3.weight   torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "linear_relu_stack.3.bias   torch.Size([64])\n",
      "torch.Size([1024, 64])\n",
      "linear_relu_stack.4.weight   torch.Size([1024, 64])\n",
      "torch.Size([1024])\n",
      "linear_relu_stack.4.bias   torch.Size([1024])\n",
      "torch.Size([11, 64])\n",
      "linear_relu_stack.5.weight   torch.Size([11, 64])\n",
      "torch.Size([11])\n",
      "linear_relu_stack.5.bias   torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "for name, param in torch_model.named_parameters():\n",
    "    print((param).shape)\n",
    "    if param.requires_grad:\n",
    "        print(name, \" \", (param.data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "88a0aaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209280"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_pytorch = nn.GRU(input_size = 1024, hidden_size =64,num_layers=1, dropout=0.3, bidirectional=False)\n",
    "sum([p.numel() for p in gru_pytorch.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "af43108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): GRU(1024, 64, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "    (1): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (3): Linear(in_features=64, out_features=11, bias=True)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.GRU(input_size = 1024, hidden_size =64,num_layers=2, dropout=0.3, bidirectional=True),\n",
    "            nn.ModuleList([\n",
    "                nn.Linear(128,64),\n",
    "                nn.Linear(64,64),\n",
    "                nn.Linear(64,64)]\n",
    "            ),\n",
    "            nn.Linear(64,1),\n",
    "            nn.Linear(64,11),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "torch_model = NeuralNetwork()\n",
    "print(count_parameters(torch_model))\n",
    "torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cbb6fcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_112 (Bidirect  (None, None, 128)        418560    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_113 (Bidirect  (None, 128)              74496     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_236 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_237 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_238 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 1)                 65        \n",
      "                                                                 \n",
      " dense_240 (Dense)           (None, 64)                0 (unused)\n",
      "                                                                 \n",
      " dense_241 (Dense)           (None, 11)                715       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 510,412\n",
      "Trainable params: 510,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(None,1024)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    tf.keras.layers.Dense(64, dynamic=True ),\n",
    "    tf.keras.layers.Dense(11,input_dim=64,input_shape=(None,64)),\n",
    "])\n",
    "gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7908f64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37632\n",
      "Sequential(\n",
      "  (0): GRU(64, 32, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy\n",
    "def count_parameters(model):\n",
    "    total_param = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_param = numpy.prod(param.size())\n",
    "            #if param.dim() > 1:\n",
    "            #    print(name, ':', 'x'.join(str(x) for x in list(param.size())), '=', num_param)\n",
    "            #else:\n",
    "            #    print(name, ':', num_param)\n",
    "            total_param += num_param\n",
    "    return total_param\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25, batch_first=True))\n",
    "print(count_parameters(model))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d008e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_28 (Bidirecti  (None, None, 64)         18816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_29 (Bidirecti  (None, None, 64)         18816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,632\n",
      "Trainable params: 37,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(None, 64)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, return_sequences=True, dropout=0.25, time_major=False)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, return_sequences=True, dropout=0.25, time_major=False)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4724b7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at Kaggle_model/GraphCodeBert and are newly initialized: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"Kaggle_model/GraphCodeBert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb3eba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0883fe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124697433"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, numpy\n",
    "def count_parameters(model):\n",
    "    total_param = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_param = numpy.prod(param.size())\n",
    "            #if param.dim() > 1:\n",
    "            #    print(name, ':', 'x'.join(str(x) for x in list(param.size())), '=', num_param)\n",
    "            #else:\n",
    "            #    print(name, ':', num_param)\n",
    "            total_param += num_param\n",
    "    return total_param\n",
    "count_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
